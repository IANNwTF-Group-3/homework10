{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install tensorflow_text, if executed in google colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  !pip install -q -U \"tensorflow-text==2.8.*\""
      ],
      "metadata": {
        "id": "mWGHyYMNt6Pz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChR6_nWXrDkf",
        "outputId": "74e0790c-6ae7-4a63-e0cc-7623b009737c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  []\n"
          ]
        }
      ],
      "source": [
        "# disable compiler warnings\n",
        "import os\n",
        "\n",
        "# imports \n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from typing import List\n",
        "import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
        "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "NLE-EgLSKVqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Load file from remote, if notebook is executed inside google colab, otherwise it gets loaded from the local file system\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  bible_url = \"https://raw.githubusercontent.com/IANNwTF-Group-3/homework10/main/bible.txt\"\n",
        "  response = requests.get(bible_url)\n",
        "  text = response.text\n",
        "else:\n",
        "  file_path = \"bible.txt\"\n",
        "  with open(file_path, \"r\") as f:\n",
        "      text = f.read()"
      ],
      "metadata": {
        "id": "4o0efQkZrfhE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "ziclBuBJ-_8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Lowercase the text\n",
        "text = text.lower()\n",
        "# Remove special characters\n",
        "for c in \"!'()*,-.0123456789:;?\":\n",
        "  text = text.replace(c, '')\n",
        "\n",
        "# Replace new lines and multiple spaces with a single space\n",
        "text = re.sub('\\n', ' ', text)\n",
        "text = re.sub(' +', ' ', text)\n",
        "print(text[:197])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAYzZyc4-xEL",
        "outputId": "26108230-d326-404a-c0a6-fc63ad750be0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the first book of moses called genesis in the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize"
      ],
      "metadata": {
        "id": "jZGQiaEa_E62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cuFe8kPMrDkk"
      },
      "outputs": [],
      "source": [
        "splitter = tf_text.RegexSplitter()\n",
        "\n",
        "splitted = splitter.split(text)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                                                 num_words=10000,\n",
        "                                                 filters=\"\\n!'()*,-.0123456789:;?\",\n",
        "                                                 lower=True,\n",
        "                                                 split=' ',\n",
        "                                                 char_level=False,\n",
        "                                                 oov_token=None,\n",
        "                                                 )\n",
        "\n",
        "tokenizer.fit_on_texts([text])\n",
        "#print(tokenizer.word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create input-taget pairs"
      ],
      "metadata": {
        "id": "7e0NVMas_Ix6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_count = 10000\n",
        "window_size = 5 # Please use odd number\n",
        "\n",
        "dataset = text.split(\" \")[:word_count]\n",
        "pairs = []\n",
        "offset = (int) (window_size / 2)\n",
        "for i in range(offset, word_count - offset):\n",
        "  for j in range(i - offset, i + offset + 1):\n",
        "    if j != i:\n",
        "      pairs.append((dataset[i], dataset[j]))\n",
        "\n",
        "print(dataset[:10])\n",
        "print(pairs[:10])"
      ],
      "metadata": {
        "id": "vgWCuk0C9Unb",
        "outputId": "df4fbc0b-3b73-4f27-e14e-fbe59a86e479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'first', 'book', 'of', 'moses', 'called', 'genesis', 'in', 'the', 'beginning']\n",
            "[('book', 'the'), ('book', 'first'), ('book', 'of'), ('book', 'moses'), ('of', 'first'), ('of', 'book'), ('of', 'moses'), ('of', 'called'), ('moses', 'book'), ('moses', 'of')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Julians Stuff"
      ],
      "metadata": {
        "id": "Y6GKq75x7oE0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dsx-TMyNrDkl",
        "outputId": "67d2d583-976b-4230-de84-ded632a4c529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vocabulary = {x: i for i, x in enumerate(np.unique(list(text)))}\n",
        "char_tokens = [vocabulary[char] for char in text]\n",
        "print(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0XwejfMbrDkm",
        "outputId": "3b5bbe71-b73d-4f9f-c797-5ed6b0ae97e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4 0 25 15]\n",
            " [19 0 11 9]\n",
            " [4 0 4 15]\n",
            " ...\n",
            " [14 3 20 9]\n",
            " [15 18 0 20]\n",
            " [5 0 23 1]]\n"
          ]
        }
      ],
      "source": [
        "sequence_length = 4\n",
        "dataset = tf.data.Dataset.from_tensor_slices(char_tokens)\n",
        "dataset = dataset.window(sequence_length,drop_remainder=True)\n",
        "batch_windows = lambda x: x.batch(sequence_length).get_single_element()\n",
        "dataset = dataset.map(batch_windows)\n",
        "dataset = dataset.shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "for seq in dataset.take(1):\n",
        "    tf.print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sRIlPmB7rDkn"
      },
      "outputs": [],
      "source": [
        "class SkipGram(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_vocabulary, embedding_dim=64):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.v = num_vocabulary\n",
        "        self.h = embedding_dim\n",
        "    \n",
        "    def build(self,_):\n",
        "        self.embedding_mat = self.add_weight(shape=(self.v, self.h),\n",
        "                                             initializer=\"random_normal\",\n",
        "                                             trainable=True) \n",
        "        self.output_mat = self.add_weight(shape=(self.v, self.h),\n",
        "                                          initializer=\"random_normal\",\n",
        "                                          trainable=True) \n",
        "        self.output_bias = self.add_weight(shape=(self.v,),\n",
        "                                           initializer=\"random_normal\",\n",
        "                                           trainable=True)\n",
        "\n",
        "    def call(self, input_id, target_id):\n",
        "        # (batch,h) = from (v,h) select 'batch_num' v* by lookup\n",
        "        embedding_vec = tf.nn.embedding_lookup(self.embedding_mat, input_id)\n",
        "\n",
        "        # compute score vector, softmax of it and loss in one function call\n",
        "        loss = tf.nn.nce_loss(weights=self.output_mat,  # (v,h)\n",
        "                              biases=self.output_bias,  # (v,)\n",
        "                              labels=tf.expand_dims(target_id,axis=1),  # (batch,1)\n",
        "                              inputs=embedding_vec,  # (batch,h)\n",
        "                              num_sampled=1,\n",
        "                              num_classes=self.v)  \n",
        "        return tf.math.reduce_mean(loss)\n",
        "\n",
        "    def embedding(self, input_id):\n",
        "        return tf.nn.embedding_lookup(self.embedding_mat, input_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# per batch\n",
        "def train_step(model, input_batch, target_batch, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = model(input_batch, target_batch)  # call directly returns the loss\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "h8sYskEOhgPf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "\n",
        "# define model\n",
        "skipgram = SkipGram(num_vocabulary=tokenizer.word_counts)\n",
        "\n",
        "# define optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "# initialize the logger for Tensorboard visualization\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/' + current_time + '/train'    \n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
      ],
      "metadata": {
        "id": "-5LtFOaRhroG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "def nearest_neighbours(model, num_vocab, selected_word_id, val_dict, k=5):\n",
        "    cosine_similarity = lambda x,y : np.dot(x,y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "\n",
        "    # embeddings of words\n",
        "    embedding_selected_word = skipgram.embedding(tf.constant(selected_word_id))\n",
        "    embedding_every_word = skipgram.embedding(tf.constant(list(range(num_vocab))))\n",
        "\n",
        "    # fit nearest neighbours using cosine similarity and embeddings of all words\n",
        "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree', metric=cosine_similarity)\n",
        "    nbrs.fit(embedding_every_word)\n",
        "    # find k_nearest nbrs of selected words. dim:(num_selected, k)\n",
        "    id_nbrs = nbrs.kneighbors(embedding_selected_word, n_neighbors=k, return_distance=False)\n",
        "    \n",
        "    # print neighbours in words instead of id\n",
        "    for i, sel_w_id in enumerate(selected_word_id):\n",
        "        query_w = val_dict[sel_w_id]\n",
        "        neigh_w = []\n",
        "        for j in range(k):\n",
        "            neigh_w.append(val_dict[id_nbrs[i,j]])\n",
        "        print('{} {} most similar words: {}'.format(query_w, k, neigh_w))"
      ],
      "metadata": {
        "id": "eSwg6dFKiOxe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SELECTED_WORDS = [\"holy\", \"father\", \"wine\", \"poison\", \"love\", \"strong\", \"day\"] # TODO ids\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('Epoch: ', epoch)\n",
        "    train_losses = []  # each entry is averaged loss of each batch\n",
        "    # train over all batches\n",
        "\n",
        "    for input_batch, target_batch in dataset:\n",
        "        train_losses.append(train_step(skipgram, input_batch, target_batch, optimizer))\n",
        "\n",
        "    # log train loss\n",
        "    with train_summary_writer.as_default():  \n",
        "        tf.summary.scalar('loss', np.mean(train_losses), step=epoch)\n",
        "\n",
        "    # Nearest neighbours to check embeddings\n",
        "    #nearest_neighbours(skipgram, tokenizer.word_counts, SELECTED_WORDS, \"\") # TODO lookup structure to convert id to the word\n",
        "    print(' ')"
      ],
      "metadata": {
        "id": "5Vqgkhnrhwto",
        "outputId": "6628a48a-8097-42a6-ef04-1629dac70b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c15c7cbc919d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# train over all batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ki_klausur",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ccc2749a7be4b92b5b584e86e3aa803380c51e1da575f498b6f2d8cfba82568a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}